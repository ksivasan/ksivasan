Problem 2
---------

1.  Loading MNIST data

``` r
#library(R.utils)
#R.utils::gunzip("train-images-idx3-ubyte.gz")
#R.utils::gunzip("train-labels-idx1-ubyte.gz")
#R.utils::gunzip("t10k-images-idx3-ubyte.gz")
#R.utils::gunzip("t10k-labels-idx1-ubyte.gz")

load_mnist <- function() {
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  train <<- load_image_file('train-images-idx3-ubyte')
  test <<- load_image_file('t10k-images-idx3-ubyte')
  
  train$y <<- load_label_file('train-labels-idx1-ubyte')
  test$y <<- load_label_file('t10k-labels-idx1-ubyte')  
}


show_digit <- function(arr784, col=gray(12:1/12), ...) {
  image(matrix(arr784, nrow=28)[,28:1], col=col, ...)
}

load_mnist()
show_digit(train$x[10,])
```

![](hw4_v2_files/figure-markdown_github/Load%20data-1.png)

Like mentioned in the question, here we create a new dataset containing only twos and threes. This can be extended to any given label set in general.

``` r
# create new list
prob_data <- function (label_list){
    prob2 = list()
    # find position of labels 2 and 3
    prob2$interestlabel = label_list
    prob2$position = which(train$y %in% prob2$interestlabel)
    # set n
    prob2$n = length(prob2$position)
    # extract x for given y
    prob2$x = train$x[prob2$position,]
    # extract true label info
    prob2$y = train$y[prob2$position]
    # set threshold on data to make it binary without for loop
    prob2$x[prob2$x<=5] <- 0
    prob2$x[prob2$x>5] <- 1
    prob2$probability = table(prob2$y)
    prob2$probability = prob2$probability/sum(prob2$probability)
    return (prob2)
} 
```

1.  Given N observations X = (*x*<sub>1</sub>, *x*<sub>2</sub>, …*x*<sub>*N*</sub>) and their cluster assignments C = (*c*<sub>1</sub>, *c*<sub>2</sub>, …, *c*<sub>*N*</sub>), the log joint probability is calculated as follows starting with Bernoulli vector and including the mixture probabilities.
2.  If X and C are observed, we get the maximum likelihood of *π* and *μ*<sup>*k*</sup> as
3.  Explain why *P*(*C*|*X*, *π*, *μ*)=*Π*<sub>*i* = 1</sub><sup>*N*</sup>*p*(*c*<sub>*i*</sub>|*x*<sub>*i*</sub>, *π*, *μ*). Since we know the samples are independent, we know *P*(*C*|*X*, *π*, *μ*)=*Π*<sub>*i* = 1</sub><sup>*N*</sup>*P*(*c*<sub>*i*</sub>|*X*, *π*, *μ*). The probability of *i*<sup>*t**h*</sup> sample belonging to a certain class is independent of the entire dataset X but *x*<sub>*i*</sub>, *π*, *μ*. Hence, we can rewrite it as *P*(*C*|*X*, *π*, *μ*)=*Π*<sub>*i* = 1</sub><sup>*N*</sup>*P*(*c*<sub>*i*</sub>|*x*<sub>*i*</sub>, *π*, *μ*). Now,
4.  Variational lower bound $\\mathcal{F} (q,\\pi,\\mu)=\\sum\_{i=1}^{N} E\_{q\_i}\[\\text{log}(P(x\_i,c\_i|\\pi,\\mu))\] + \\sum\_{i=1}^N H(q\_i)$ where entropy *H*(*q*<sub>*i*</sub>)=∑<sub>*c*<sub>*i*</sub></sub>*q*<sub>*i*</sub>(*c*<sub>*i*</sub> = *k*)log(*q*<sub>*i*</sub>(*c*<sub>*i*</sub> = *k*)).
    $$\\mathcal{F} (q,\\pi,\\mu)=\\sum\_{i=1}^{N} \\sum^K\_{k=1}q\_{ik}\[\\text{log}(P(x\_i,c\_i=k|\\pi,\\mu))\] + \\sum\_{i=1}^N H(q\_i)$$
    $$\\mathcal{F} (q,\\pi,\\mu)=\\sum\_{i=1}^{N} \\sum^K\_{k=1}q\_{ik} \[\\text{log }\\pi\_k+ \\sum\_{j=1}^{784}\[(x\_j^i) \\text{log } \\mu\_j^k + (1-x\_j^i) \\text{log } (1-\\mu\_j^k)\]\]+ \\sum\_{i=1}^N H(q\_i)$$

5.  For this part, we set ∀*k*

For this part, we set ∀*j*, *k*
``` r
Bernoulli <- function (xvec, muvec){
  pos = muvec>0
  xvec = xvec[pos]
  muvec = muvec[pos]
  bernoulli_prob = exp(sum(xvec*log(muvec)+(1-xvec)*log(1-muvec)))
  return (bernoulli_prob)
} 
EMalgorithm <-function(prob2){
  set.seed(10)
  prob2$pred = sample(prob2$interestlabel, prob2$n, replace=TRUE)
  prob2$uniqueclass = unique(prob2$y)
  K = length(prob2$uniqueclass)
  prob2$pi = runif(K)
  prob2$pi = prob2$pi/sum(prob2$pi)
  prob2$q = matrix(rep(0,K*prob2$n), ncol=K)
  prob2$qt = matrix(rep(0,K*prob2$n), ncol=K)
  prob2$mu = matrix(sample(c(10:90),784*K, replace=TRUE), nrow = length(prob2$uniqueclass))/100
  prob2$qclass = rep(0,K)
  maxIter = 50
  prob2$FreeEnergy = rep(-10,maxIter)
  prob2$Likelihood = rep(-10,maxIter)
  prob2$EntropyIndividual = rep(1,prob2$n)
  prob2$Entropy = rep(0,maxIter)
  DiffFreeEnergy = 1000
  iter = 1
  while (iter <= maxIter && abs(DiffFreeEnergy) > 1){
    #print("E step")
    for (i in 1:prob2$n){
      for (k in 1:K){
        prob2$qt[i,k]= prob2$pi[k]*(Bernoulli(prob2$x[i,], prob2$mu[k,])) 
      }
      prob2$q[i,] = prob2$qt[i,]/sum(prob2$qt[i,])
      prob2$pred[i] = prob2$uniqueclass[which(prob2$q[i,]==max(prob2$q[i,]))]
    }
    Likelihood_prep = 0
    for (class in 1:K){
      dummy = prob2$qt[,class]
      pos = dummy > 0
      qnonzero = prob2$q[,class]
      Likelihood_prep = Likelihood_prep+sum(qnonzero[pos]*log(dummy[pos]))
    }
    prob2$Likelihood[iter] = Likelihood_prep
    for (enti in 1:prob2$n){
      nonzero_q = prob2$q[enti,1]
      nonzero_q = nonzero_q[nonzero_q>0]
      prob2$EntropyIndividual[enti] = -sum(nonzero_q*log(nonzero_q))   
    }
    prob2$Entropy[iter] = sum(prob2$EntropyIndividual)
    prob2$FreeEnergy[iter] = prob2$Likelihood[iter] + prob2$Entropy[iter]
    if (iter > 1){
      prob2$DiffFreeEnergy = prob2$FreeEnergy[iter]-prob2$FreeEnergy[iter-1] 
    }
    #print(prob2$FreeEnergy[iter])
    #print("M step")
    prob2$qclass = colSums(prob2$q)
    prob2$pi = prob2$qclass/prob2$n
    for (k in 1:K){
      prob2$mu[k,] = colSums(prob2$q[,k]*prob2$x)/prob2$qclass[k]
    }
    iter = iter+1
  }
  cat("Iteration:",iter)
  cat("\n")
  #print(prob2$pi)
  for (mu_i in 1:K){
    show_digit(prob2$mu[mu_i,])
  }
  prob2$error = prob2$pred - prob2$y
  prob2$accuracy = length(prob2$error[prob2$error==0])/length(prob2$error)
  prob2$FreeEnergy = prob2$FreeEnergy[1:iter-1]
  prob2$Likelihood = prob2$Likelihood[1:iter-1]
  prob2$Entropy = prob2$Entropy[1:iter-1]
  plot(prob2$FreeEnergy, xlab="Iteration", ylab="Variational Free Energy")
  #plot(Likelihood)
  #plot(Entropy)
  cat("Pi =",prob2$pi)
  cat("\n")
  cat("Free energy:",prob2$FreeEnergy[length(prob2$FreeEnergy)])
  cat("\n")
  prob2$EntropyIndividual = rep(1,prob2$n)
  for (ient in 1:prob2$n){
    nonzero_q = prob2$q[ient,1]
    nonzero_q = nonzero_q[nonzero_q>0]
    prob2$EntropyIndividual[ient] = -sum(nonzero_q*log(nonzero_q))   
  }
  maxEntropy = which(prob2$EntropyIndividual==max(prob2$EntropyIndividual))
  cat("The parameter vectors are printed followed by digit having maximum entropy")
  show_digit(prob2$x[maxEntropy,])
  return (prob2)
}
```

``` r
prob2 <- prob_data(c(2:3))
prob2 <- EMalgorithm(prob2)
```

    ## Iteration: 51

![](hw4_v2_files/figure-markdown_github/main1-1.png)![](hw4_v2_files/figure-markdown_github/main1-2.png)![](hw4_v2_files/figure-markdown_github/main1-3.png)

    ## Pi = 0.482404 0.517596
    ## Free energy: -2572950
    ## The parameter vectors are printed followed by digit having maximum entropy

![](hw4_v2_files/figure-markdown_github/main1-4.png)

``` r
prob3 <- prob_data(c(2:4))
prob3 <- EMalgorithm(prob3)
```

    ## Iteration: 51

![](hw4_v2_files/figure-markdown_github/main2-1.png)![](hw4_v2_files/figure-markdown_github/main2-2.png)![](hw4_v2_files/figure-markdown_github/main2-3.png)![](hw4_v2_files/figure-markdown_github/main2-4.png)

    ## Pi = 0.336032 0.3154327 0.3485353
    ## Free energy: -3709032
    ## The parameter vectors are printed followed by digit having maximum entropy

![](hw4_v2_files/figure-markdown_github/main2-5.png)

The units of F- Variational Free energy is same as that of entropy. Entropy can be defined as the measurement of information carried by a probability distribution. It is also the average rate at which information is produced by a stochastic source of data. The unit of entropy depends on the base of the logarithm that is defined to calculate entropy. In our case, we used natural logarithm (base of e) and hence we have the unit as nat for Entropy and variational Free Energy. In general, if n outcomes are possible for an event, then *l**o**g*<sub>2</sub>(*n*) bits would be required to represent the variable defining the event. In this problem, we have N independent samples with each possible of taking K class values. Hence, *N*<sup>*K*</sup> possibilities exist and entropy gives an expected value for how many nats (or appropriate unit) required for defining the outcome of this event.
